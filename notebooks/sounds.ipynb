{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Sounds from Swarm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Currently generating sound from MAG_LR (F)\n",
    "\n",
    "Sound processing based on work by Nikolai Linden-VÃ¸rnle: https://gitlab.gbar.dtu.dk/s183730/sonification-ESA-Swarm/\n",
    "\n",
    "References:\n",
    "\n",
    "- https://resampy.readthedocs.io/\n",
    "- https://pyrubberband.readthedocs.io/\n",
    "- https://panel.holoviz.org/reference/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from time import sleep\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from hapiclient import hapi, hapitime2datetime\n",
    "import resampy\n",
    "import pyrubberband as pyrb\n",
    "\n",
    "import panel as pn\n",
    "\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tfalib_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Data access through VirES+HAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(t0, t1) -> pd.DataFrame:\n",
    "    \"\"\"Fetch data from VirES HAPI\n",
    "    \"\"\"\n",
    "    if isinstance(t0, dt.datetime):\n",
    "        t0 = t0.isoformat()\n",
    "        t1 = t1.isoformat()\n",
    "    data, meta = hapi(\n",
    "        \"https://vires.services/hapi/\",\n",
    "        \"SW_OPER_MAGA_LR_1B\",\n",
    "        \"Latitude,Longitude,Radius,F\",\n",
    "        t0,\n",
    "        t1,\n",
    "    )\n",
    "    # Convert to dataframe\n",
    "    #  To fix: this will not work with vector, e.g. B_NEC\n",
    "    df = pd.DataFrame(columns=data.dtype.names, data=data)\n",
    "    df = df.set_index(\"Timestamp\")\n",
    "    df.index = hapitime2datetime(df.index.values.astype(str))\n",
    "    df.index = df.index.tz_convert(\"UTC\").tz_convert(None)\n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_data_padded(t0, t1):\n",
    "    # padding is required for the TFA toolbox\n",
    "    t0 = t0 - dt.timedelta(hours=3)\n",
    "    t1 = t1 + dt.timedelta(hours=3)\n",
    "    return fetch_data(t0, t1)\n",
    "\n",
    "\n",
    "t0, t1 = dt.datetime(2020, 3, 14), dt.datetime(2020, 3, 14, 1)\n",
    "df = fetch_data_padded(t0, t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Sound processing tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(x):\n",
    "    \"\"\"Normalise signal to +1/-1 range\"\"\"\n",
    "    return (x - np.average(x)) / (np.ptp(x))\n",
    "\n",
    "\n",
    "def highpass_filter(x, filter_order=8, cutoff_freq=0.0001, fs=1):\n",
    "    \"\"\"Apply butterworth highpass filter to remove DC offset\"\"\"\n",
    "    sos = signal.butter(filter_order, cutoff_freq, 'highpass', fs=fs, output='sos')\n",
    "    # w, h = signal.sosfreqz(sos)\n",
    "    return signal.sosfilt(sos, x)\n",
    "\n",
    "\n",
    "def resample(x, fs=44100, resampling_factor=7):\n",
    "    \"\"\"Resample to new frequency\"\"\"\n",
    "    sr_new = int(fs / resampling_factor)\n",
    "    return resampy.resample(x, fs, sr_new)\n",
    "\n",
    "\n",
    "def smooth_edges(x, fraction_fade=0.1, t_fade=0.1, fs=44100):\n",
    "    \"\"\"Smoothing window to avoid clicks and pops at the start and end of the signal\n",
    "\n",
    "    If fraction_fade provided, use that to apply to a fraction of the signal,\n",
    "    otherwise, set fraction_fade=None to use t_fade instead\n",
    "    \"\"\"\n",
    "    window = np.ones(len(x))\n",
    "    if fraction_fade:\n",
    "        length_fade = int(len(x)*fraction_fade)\n",
    "    else:\n",
    "        length_fade = int(t_fade * fs)\n",
    "    # ramp up the fade from 0 to 1 over the duration of length_fade\n",
    "    fade = np.linspace(0, 1, length_fade)\n",
    "    window[:length_fade] = fade\n",
    "    window[-length_fade:] = fade[::-1]\n",
    "    return x * window\n",
    "\n",
    "\n",
    "def time_stretch(x, fs=44100, target_length=10):\n",
    "    \"\"\"Stretch duration to target_length (seconds)\"\"\"\n",
    "    input_length = len(x) / fs\n",
    "    ts_ratio = input_length / target_length\n",
    "    return pyrb.time_stretch(x, fs, ts_ratio)\n",
    "\n",
    "\n",
    "def pitch_shift(x, fs=44100, octaves=1):\n",
    "    \"\"\"Shift the pitch by a given number of octaves\"\"\"\n",
    "    return pyrb.pitch_shift(x, fs, 12*octaves)\n",
    "\n",
    "\n",
    "def sound_format(x):\n",
    "    \"\"\"Convert to int16 for audio output\"\"\"\n",
    "    return np.int16(x*32767)\n",
    "\n",
    "\n",
    "def get_sound_pane(x, fs=44100):\n",
    "    \"\"\"Generate panel Audio pane\"\"\"\n",
    "    return pn.pane.Audio(x, sample_rate=fs, sizing_mode=\"stretch_width\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(x, fs=1, resampling_factor=1, ymax=0.00125, figsize=(5, 5)):\n",
    "    nperseg = 2**(16 + (1 - (int(0.5 + resampling_factor/2))))\n",
    "    if nperseg > len(x):\n",
    "        nperseg = 10000\n",
    "    f, t, Sxx = signal.spectrogram(x, fs, mode='magnitude', nperseg=nperseg)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    ax.pcolormesh(t, f, Sxx[:], shading='gouraud', cmap='hot')\n",
    "    ax.set_ylabel('Frequency [Hz]')\n",
    "    ax.set_xlabel('Time [sec]')\n",
    "    ax.set_ylim(0, ymax)\n",
    "    plt.close()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Now apply a pipeline created from the functions above..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_audio_pipeline(x, target_length=10, shift_octaves=1):\n",
    "    x = normalise(x)\n",
    "    x = highpass_filter(x, filter_order=8, cutoff_freq=0.0001, fs=1)\n",
    "    x = resample(x, fs=44100, resampling_factor=7)\n",
    "    x = smooth_edges(x, fraction_fade=0.1)  # fs=44100, t_fade=0.1)\n",
    "    x = time_stretch(x, fs=44100, target_length=target_length)\n",
    "    x = pitch_shift(x, fs=44100, octaves=shift_octaves)\n",
    "    # plot_spectrogram(x, fs=44100, resampling_factor=7, ymax=800)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_spectrogram(df[\"F\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "premade_audio_data = _apply_audio_pipeline(df[\"F\"].values, target_length=10, shift_octaves=1)\n",
    "# plot_spectrogram(premade_audio_data, fs=44100, resampling_factor=7, ymax=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_sound_pane(\n",
    "#     sound_format(premade_audio_data),\n",
    "#     fs=44100\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Wavelet analysis from SwarmX:TFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_using_wavelets(df, t0, t1, cutoff=20/1000, minScale=10, maxScale=100, dj=0.1):\n",
    "    tfa_params = {'time_lims': [t0, t1]}\n",
    "    tfa = tfalib_temp.TempTFAdata(df.index, df[\"F\"].values.reshape(len(df[\"F\"].values), 1), params=tfa_params)\n",
    "    cadence = tfalib_temp.Cadence({'Sampling_Rate': 86400, 'Interp': False})\n",
    "    cleaning = tfalib_temp.Cleaning({'Window_Size': 50, 'Method': 'iqr', 'Multiplier': 6})\n",
    "    filtering = tfalib_temp.Filtering({'Sampling_Rate': 1, 'Cutoff': cutoff, })\n",
    "    wavelet = tfalib_temp.Wavelet(\n",
    "        {'Time_Step': 1, 'Min_Scale': minScale, \n",
    "         'Max_Scale': maxScale, 'dj': dj}\n",
    "    )\n",
    "    cadence.apply(tfa)\n",
    "    cleaning.apply(tfa)\n",
    "    filtering.apply(tfa)\n",
    "    wavelet.apply(tfa)\n",
    "    return tfa\n",
    "\n",
    "# tfa = transform_using_wavelets(df, t0, t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfa.image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_tfa = tfa.wave_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_data = _apply_audio_pipeline(x_tfa)\n",
    "# plot_spectrogram(audio_data, fs=44100, resampling_factor=7, ymax=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_sound_pane(\n",
    "#     sound_format(audio_data),\n",
    "#     fs=44100\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Extend the audio pipeline to handle the dataframe and transformations too..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_audio_pipeline_to_df(df, t0, t1, target_length=10, shift_octaves=1, transform=False, transform_kwargs=dict()):\n",
    "    if transform:\n",
    "        tfa = transform_using_wavelets(df, t0, t1, **transform_kwargs)\n",
    "        x = tfa.wave_index()\n",
    "    else:\n",
    "        x = df[\"F\"].values\n",
    "        tfa = None\n",
    "    x = _apply_audio_pipeline(x, target_length=target_length, shift_octaves=shift_octaves)\n",
    "    return x, tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, tfa = apply_audio_pipeline_to_df(df, t0, t1, target_length=10, shift_octaves=1, transform=False)\n",
    "# get_sound_pane(\n",
    "#     sound_format(x),\n",
    "#     fs=44100\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, tfa = apply_audio_pipeline_to_df(\n",
    "#     df, t0, t1, target_length=10, shift_octaves=1, transform=True,\n",
    "#     transform_kwargs=dict(cutoff=20/1000, minScale=10, maxScale=100, dj=0.1)\n",
    "# )\n",
    "# get_sound_pane(\n",
    "#     sound_format(x),\n",
    "#     fs=44100\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Panel dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDashboard:\n",
    "    def __init__(self, df=df, t0=t0, t1=t1):\n",
    "        \"\"\"Initialise with the data from above, and create Panel objects\"\"\"\n",
    "        self.default_t0t1 = t0, t1\n",
    "        self.t0 = t0\n",
    "        self.t1 = t1\n",
    "        # self.df = fetch_data(t0, t1)\n",
    "        self.df = df  # use the pre-fetched data from above for speed\n",
    "        # self.audio_data = apply_audio_pipeline(self.df[\"F\"].values)\n",
    "        self.audio_data = premade_audio_data  # use the premade data from above for speed\n",
    "        spectrogram_in = plot_spectrogram(self.df[\"F\"].values)\n",
    "        spectrogram_out = plot_spectrogram(self.audio_data, fs=44100, resampling_factor=7, ymax=800)\n",
    "        wavfilename = self.write_file_for_download()\n",
    "        self.widgets = {\n",
    "            \"time_range\": pn.widgets.DatetimeRangeInput(\n",
    "                start=dt.datetime(2013, 1, 1, 0, 0, 0), end=dt.datetime(2023, 1, 1, 0, 0, 0),\n",
    "                value=(t0, t1)\n",
    "            ),\n",
    "            \"button1\": pn.widgets.Button(\n",
    "                name=\"Fetch data\", button_type=\"primary\"\n",
    "            ),\n",
    "            \"loading1\": pn.indicators.Progress(active=False, sizing_mode=\"stretch_width\"),\n",
    "            \"wavelet_transform_checkbox\": pn.widgets.Checkbox(name=\"Apply wavelet transform:\"),\n",
    "            \"target_length\": pn.widgets.IntSlider(\n",
    "                name=\"Output length (seconds)\",\n",
    "                start=1, end=60, step=1, value=10\n",
    "            ),\n",
    "            \"shift_octaves\": pn.widgets.IntSlider(\n",
    "                name=\"Shift by number of octaves\",\n",
    "                start=0, end=6, value=1\n",
    "            ),\n",
    "            \"button2\": pn.widgets.Button(\n",
    "                name=\"Regenerate sound â¡ï¸\", button_type=\"primary\"\n",
    "            ),\n",
    "            \"loading2\": pn.indicators.Progress(active=False, sizing_mode=\"stretch_width\"),\n",
    "            \"file_download\": pn.widgets.FileDownload(file=wavfilename)\n",
    "        }\n",
    "        self.widgets_wavelet_transform = {\n",
    "            \"cutoff\": pn.widgets.EditableFloatSlider(name=\"Cutoff frequency\", start=0.001, end=0.1, value=0.020, step=0.001),\n",
    "            \"minScale\": pn.widgets.EditableFloatSlider(name=\"Wavelet smallest scale\", start=1, end=100, value=10, step=10),\n",
    "            \"maxScale\": pn.widgets.EditableFloatSlider(name=\"Wavelet largest scale\", start=100, end=1000, value=100, step=10),\n",
    "            \"dj\": pn.widgets.EditableFloatSlider(name=\"Step size for scales for wavelet transform\", start=0.05, end=0.15, value=0.1, step=0.01),\n",
    "        }\n",
    "        self.panes = {\n",
    "            \"audio\": get_sound_pane(sound_format(self.audio_data)),\n",
    "            \"spectrogram_in\": pn.pane.Matplotlib(spectrogram_in),\n",
    "            \"spectrogram_out\": pn.pane.Matplotlib(spectrogram_out),\n",
    "            \"wavelets_spectrogram\": pn.pane.Matplotlib()\n",
    "        }\n",
    "        self.widgets[\"button1\"].on_click(self.update_data)\n",
    "        self.widgets[\"button2\"].on_click(self.update_audio)\n",
    "\n",
    "    def update_data(self, event):\n",
    "        \"\"\"Fetch the data from VirES and reset the dashboard\"\"\"\n",
    "        t0, t1 = self.widgets[\"time_range\"].value\n",
    "        self.t0, self.t1 = t0, t1\n",
    "        if (t1 - t0) > dt.timedelta(days=7):\n",
    "            self.widgets[\"button1\"].name = \"Time > 7 days not allowed !\"\n",
    "            self.widgets[\"time_range\"].value = self.default_t0t1\n",
    "            sleep(3)\n",
    "            self.widgets[\"button1\"].name = \"Fetch data\"\n",
    "            return None\n",
    "        self.widgets[\"button1\"].name = \"Busy...\"\n",
    "        self.widgets[\"loading1\"].active = True\n",
    "        self.df = fetch_data_padded(t0, t1)\n",
    "        try:\n",
    "            spectrogram_in = plot_spectrogram(self.df[\"F\"].values)\n",
    "        except KeyError:\n",
    "            self.widgets[\"button1\"].name = \"Missing data for this time...\"\n",
    "            self.widgets[\"time_range\"].value = self.default_t0t1\n",
    "            sleep(3)\n",
    "            self.widgets[\"button1\"].name = \"Fetch data\"\n",
    "            self.widgets[\"loading1\"].active = False\n",
    "            return None\n",
    "        self.panes[\"spectrogram_in\"].object = spectrogram_in\n",
    "        self.widgets[\"button1\"].name = \"Fetch data\"\n",
    "        self.widgets[\"loading1\"].active = False\n",
    "        self.update_audio(None)\n",
    "\n",
    "    def update_audio(self, event):\n",
    "        try:\n",
    "            self._update_audio()\n",
    "        except Exception:\n",
    "            self.widgets[\"button2\"].name = \"Oops! Something went wrong\"\n",
    "            self.widgets[\"loading2\"].active = False\n",
    "\n",
    "    def _update_audio(self):\n",
    "        \"\"\"Update the output spectrogram and audio\"\"\"\n",
    "        self.panes[\"audio\"].paused = True\n",
    "        self.widgets[\"button2\"].name = \"Busy...\"\n",
    "        self.panes[\"wavelets_spectrogram\"].object = None\n",
    "        # Change contents of audio and spectrogram\n",
    "        self.widgets[\"loading2\"].active = True\n",
    "        # Extract kwargs to pass to the wavelet transform\n",
    "        wavelet_transform_kwargs = {k: v.value for k, v in self.widgets_wavelet_transform.items()}\n",
    "        # Apply the pipeline\n",
    "        x, tfa = apply_audio_pipeline_to_df(\n",
    "            self.df,\n",
    "            self.t0,\n",
    "            self.t1,\n",
    "            target_length=self.widgets[\"target_length\"].value,\n",
    "            shift_octaves=self.widgets[\"shift_octaves\"].value,\n",
    "            transform=self.widgets[\"wavelet_transform_checkbox\"].value,\n",
    "            transform_kwargs=wavelet_transform_kwargs\n",
    "        )\n",
    "        # Insert the wavelet spectrogram\n",
    "        if tfa:\n",
    "            self.panes[\"wavelets_spectrogram\"].object = tfa.image()\n",
    "        # Update the audio\n",
    "        self.audio_data = x\n",
    "        self.panes[\"audio\"].object = sound_format(x)\n",
    "        spectrogram_out = plot_spectrogram(x, fs=44100, resampling_factor=7, ymax=800)\n",
    "        self.panes[\"spectrogram_out\"].object = spectrogram_out\n",
    "        self.write_file_for_download()\n",
    "        # Reset button & loading widget\n",
    "        self.widgets[\"button2\"].name = \"Regenerate sound â¡ï¸\"\n",
    "        self.widgets[\"loading2\"].active = False\n",
    "\n",
    "    def write_file_for_download(self, filename=\"sonification.wav\"):\n",
    "        wavfile.write(filename, 44100, sound_format(self.audio_data))\n",
    "        return filename\n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"GridSpec-based layout of all the widgets and panes\"\"\"\n",
    "        gspec = pn.GridSpec(sizing_mode=\"stretch_both\", max_height=800)\n",
    "        gspec[:, 0] = pn.Column(\n",
    "            pn.pane.Markdown(\"## 1. Input data\"),\n",
    "            pn.pane.Markdown(\"`SW_OPER_MAGA_LR_1B: F`\"),\n",
    "            self.widgets[\"time_range\"],\n",
    "            self.widgets[\"button1\"],\n",
    "            self.widgets[\"loading1\"],\n",
    "            pn.pane.Markdown(\"### Input spectrogram\"),\n",
    "            self.panes[\"spectrogram_in\"]\n",
    "        )\n",
    "        gspec[:, 1] = pn.Column(\n",
    "            pn.pane.Markdown(\"## 2. Transformations\"),\n",
    "            pn.layout.spacer.VSpacer(max_height=10),\n",
    "            pn.Column(\n",
    "                self.widgets[\"wavelet_transform_checkbox\"],\n",
    "                *self.widgets_wavelet_transform.values(),\n",
    "                pn.pane.Markdown(\"*To apply these, tick the checkbox above*\"),\n",
    "                background=\"WhiteSmoke\",\n",
    "                align=\"center\"\n",
    "            ),\n",
    "            pn.layout.spacer.VSpacer(max_height=10),\n",
    "            pn.Column(\n",
    "                self.widgets[\"target_length\"],\n",
    "                self.widgets[\"shift_octaves\"],\n",
    "                self.widgets[\"button2\"],\n",
    "                background=\"WhiteSmoke\",\n",
    "                align=\"center\"\n",
    "            ),\n",
    "            pn.layout.spacer.VSpacer(max_height=10),\n",
    "            self.panes[\"wavelets_spectrogram\"],\n",
    "            background=\"Snow\"\n",
    "        )\n",
    "        gspec[:, 2] = pn.Column(\n",
    "            pn.pane.Markdown(\"## 3. Output spectrogram & audio\"),\n",
    "            self.widgets[\"loading2\"],\n",
    "            self.panes[\"spectrogram_out\"],\n",
    "            self.panes[\"audio\"],\n",
    "            self.widgets[\"file_download\"]\n",
    "        )\n",
    "        return gspec\n",
    "\n",
    "\n",
    "SoundDashboard().display().servable(\"Sounds from Swarm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:funswarm2]",
   "language": "python",
   "name": "conda-env-funswarm2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
